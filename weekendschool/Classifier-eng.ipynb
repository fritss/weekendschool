{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random as rd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from hulp_functies import plot_model, plot_images, generate_metadata\n",
    "rd.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create directories where you can put the photos\n",
    "The code in this cell creates a directory `camera` containing two sub-directories: `objects` and `background`. In the sub-directory `objects` you can create new sub-directories for each object that you are going to photograph (say 2-5 sub-directories). In these own sub-directories you have to put about 35 different images of your object. In the sub-directory `background` you have to put about 60 images without the objects. Don't spend too much time taking the pictures. The pictures don't have to be artistic, they just have to be clear and from different angles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_dir = Path('camera')\n",
    "object_dir = camera_dir / 'objects'\n",
    "background_fn = 'background'\n",
    "\n",
    "camera_dir.mkdir(exist_ok=True)\n",
    "object_dir.mkdir(exist_ok=True)\n",
    "(camera_dir / background_fn).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize the pictures\n",
    "Ensure that the captured images are the size of the neural network (224 x 224 x 3). And split the pictures of the objects into a training set and a test set.\n",
    "\n",
    "The code in the next two cells reads the images from the `camera` folder, resizes them to the correct format and splits them into a `train` folder and a `test` folder. The result will be in the folder `images_224`.\n",
    "\n",
    "The images from `train` are used to train the neural network, the images from `test` are used to see how well the training has been successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 224\n",
    "channels = 3\n",
    "input_shape = (size, size, channels)\n",
    "\n",
    "object_fn = Path('objects.txt')\n",
    "resize_dir = Path(f'images_{size}')\n",
    "\n",
    "classes = os.listdir(object_dir)\n",
    "n_classes = len(classes)\n",
    "class_nums = {c:i for i,c in enumerate(classes)}\n",
    "\n",
    "with open(object_fn, 'w') as f:\n",
    "    f.write('\\n'.join(classes))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(resize_dir / background_fn).mkdir(exist_ok=True, parents=True)\n",
    "for c in classes:\n",
    "    (resize_dir / 'train' / c).mkdir(exist_ok=True, parents=True)\n",
    "    (resize_dir / 'test'  / c).mkdir(exist_ok=True, parents=True) \n",
    "\n",
    "for c in classes:\n",
    "    ims = os.listdir(object_dir / c)\n",
    "    rd.shuffle(ims)\n",
    "    Ntest = len(ims) // 5\n",
    "    for i, im in enumerate(ims):\n",
    "        dir_name = 'test' if i < Ntest else 'train'\n",
    "        image = cv2.imread(str(object_dir / c / im))\n",
    "        image_resized = cv2.resize(image, (size, size))\n",
    "        cv2.imwrite(str(resize_dir / dir_name / c / f'{str(i)}.png'), image_resized)\n",
    "            \n",
    "for i, im in enumerate(os.listdir(camera_dir / background_fn)):\n",
    "    image = cv2.imread(str(camera_dir / background_fn / im))\n",
    "    image_resized = cv2.resize(image, (size, size))\n",
    "    cv2.imwrite(str(resize_dir / background_fn / f'{str(i)}.png'), image_resized)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make the directories\n",
    "During the training we make a number of models and a number of graphs. These models and graphs are placed in separate directories. The code in the next cell creates those directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_name = 'model_mobilenet.h5'\n",
    "lite_name = 'model_mobilenet.tflite'\n",
    "json_name = 'model_mobilenet.json'\n",
    "\n",
    "model_dir = Path('models')\n",
    "saved_model_dir = Path('saved_models')\n",
    "export_model_dir = Path('export_models')\n",
    "plot_dir = Path('plots')\n",
    "\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "saved_model_dir.mkdir(exist_ok=True)\n",
    "export_model_dir.mkdir(exist_ok=True)\n",
    "plot_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the data\n",
    "\n",
    "The images you placed in the directory `camera` have been resized and put in the directory `images_224`. The next two cells read in these images and put them in an array in the working memory of your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_objects(p):\n",
    "    images = {}\n",
    "    \n",
    "    for c in classes:\n",
    "        class_dir = p / c\n",
    "        im_names = os.listdir(class_dir)\n",
    "        images[c] = np.zeros((len(im_names), size, size, channels))\n",
    "        for i, img in enumerate(im_names):\n",
    "            im = np.asarray(Image.open(class_dir / img))\n",
    "            images[c][i] = im/255\n",
    "\n",
    "    data = np.concatenate([images[c] for c in classes], axis=0)\n",
    "    labels = []\n",
    "    for c in classes:\n",
    "        labels += [class_nums[c]]*len(images[c])\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return data, labels, to_categorical(labels, n_classes)\n",
    "\n",
    "def import_background(p, N=60):\n",
    "    im_names = os.listdir(p)\n",
    "    NN = min(N, len(im_names))\n",
    "\n",
    "    npd_images = np.zeros((NN, size, size, channels))\n",
    "    for i, img in enumerate(im_names[:NN]):\n",
    "        im = np.asarray(Image.open(p / img))\n",
    "        npd_images[i] = im/255\n",
    "    class_npd = np.full((npd_images.shape[0], n_classes), 1/n_classes)\n",
    "\n",
    "    return npd_images, class_npd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, training_labels, class_train = import_objects(resize_dir / 'train')\n",
    "test_data, test_labels, class_test = import_objects(resize_dir / 'test')\n",
    "\n",
    "Ntrain = training_data.shape[0]\n",
    "Ntest = test_data.shape[0]\n",
    "\n",
    "npd_train_images, class_npd_train = import_background(resize_dir / background_fn, N=2*Ntrain)\n",
    "\n",
    "training_data_ext = np.concatenate((training_data, npd_train_images), axis=0)\n",
    "class_train_ext = np.concatenate((class_train, class_npd_train), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a model\n",
    "We take a standard model: Mobilenet. This model is designed especially for mobile devices such as smartphones. The only thing left to tell this model is how many types of objects it should be able to distinguish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = applications.MobileNet(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "\n",
    "predictions = Dense(n_classes, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A function to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(m, tr_data, tr_class, bs, epochs, lr=1.0e-4, fn=None):\n",
    "    cp = ModelCheckpoint(str(saved_model_dir / fn),\n",
    "                         monitor='val_loss',\n",
    "                         verbose=0, # verbosity - 0 or 1\n",
    "                         save_best_only= True,\n",
    "                         mode='auto')\n",
    "\n",
    "    m.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate=lr),\n",
    "              metrics = ['accuracy'])\n",
    "    \n",
    "    details = m.fit(tr_data, tr_class,\n",
    "                    batch_size = bs,\n",
    "                    epochs = epochs,\n",
    "                    shuffle = True,\n",
    "                    validation_data= (test_data, class_test),\n",
    "                    callbacks=[cp],\n",
    "                    verbose=1)\n",
    "    if not fn is None:\n",
    "        m.save(str(model_dir / fn))\n",
    "    return details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model\n",
    "Now comes the real work: training the model. The batch size is 32, which means that the model looks at 32 images each time and adjusts its weights accordingly. The number of epochs is 6, which means that all images are viewed 6 times.\n",
    "\n",
    "This step can take a long time, depending on the computing power of your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 6\n",
    "\n",
    "model_details = train_model(model, training_data_ext, class_train_ext, bs=batch_size, epochs=epochs, fn=keras_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the training\n",
    "These graphs show how good the model was after each epoch (an epoch was looking at all training images once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_details, plot_dir / \"model_details.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the result\n",
    "We can see how good the model is on the test pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_pred = model.predict(test_data)\n",
    "labels_pred = np.argmax(class_pred,axis=1)\n",
    "print(f'accuracy on test set: {100*np.mean(labels_pred==test_labels):.2f}%')\n",
    "\n",
    "idx = rd.sample(range(Ntest), 12)\n",
    "plot_images(test_data[idx], test_labels[idx], classes, labels_pred[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the model to a tensorflow lite model\n",
    "In order to use the model in an Android app, it must be converted to a slightly different format. The weights in the model get slightly fewer bits, making the model a little smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "(model_dir / lite_name).write_bytes(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add metadata \n",
    "The Android app still needs to know a few things about the model, such as what the input format is and how many types of objects it should be able to distinguish. This information comes in a separate json file. You can also enter your own name here as the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_INFO = {\n",
    "    'author' : '<your own name between quotes>',\n",
    "    'size' : size,\n",
    "    'classes' : classes,\n",
    "    'label_fn' : str(object_fn),\n",
    "    'label_path' : object_fn.resolve(),\n",
    "    'model_path' : (model_dir / lite_name).resolve(),\n",
    "    'export_model_path' : (export_model_dir / lite_name).resolve(),\n",
    "    'json_fn' : export_model_dir / json_name\n",
    "}\n",
    "generate_metadata(MODEL_INFO)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
